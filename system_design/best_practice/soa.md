One of the unique strengths at Amazon is that individual teams are given tremendous latitude to make their own technology choices. One of the major trends of 2005/2006 is decoupling: the creation of technologies which reduce explicit, required dependencies between components. This has affected all levels of the technology stack, from the HTML generation layer to the software development tools.
One of the consequences of this freedom is that there are not many firm guidelines for the “right” way to do things. As such, you will need to talk with the people in your group to see how closely what is described here matches what your group, in particular, is doing.
Before digging into the specifics described here, it is strongly recommended that you take a spin through the Boot Camp documentation. There's a lot of information, and it's very dense; but you really need to struggle through it.

Contents[hide]

   * 1 Planning for Scale
      * 1.1 SLA
      * 1.2 Data locality
      * 1.3 Expected scale points
      * 1.4 CAP Theorem
      * 1.5 Consistency techniques
      * 1.6 Caching techniques
      * 1.7 Partitioning techniques
      * 1.8 Recovery Oriented Computing

# Planning for Scale
Obviously, this is a huge topic about which books have been written. This is intended only as a brief introduction to the vocabulary we use at Amazon.

## SLA
The "service level agreement" is a tremendously overloaded term which ultimately covers everything from overall service availability, to expected request volume that can be handled, to the expected delay between data entry and availability.
In the end analysis, your "SLA" is the contract you effectively make with the consumers of the data you provide, be they the Amazon customer or a mediating service. Understanding the contract you expect to honor is the central design consideration, and as such you need to understand the SLA for your service.
The Boot Camp documentation has some interesting things to say about this as well.

## Data locality
Thinking quite abstractly for a moment, it can be said that all the thinking and research about Service Oriented Architecture is focused on getting three pieces of information to the same place:

  1. A corpus of data, which is the bunch of bits which represents the latent data in which people may be interested -- eg, the customer database, the order database, etc.
  2. A request, generated by an end client or a mediating client service, which is a stream of bits which is routed across a network indicating a demand for work.
  3. The code for your service, which is a bunch of bits that happen to do the appropriate work demanded by a request in context of the corpus.

These three then collaborate to create a fourth piece of information:

  1. A response, which is routed back across the network to the originating server.

Traditionally, architectures have assumed that the mobility of these are primarily related to their relative size, and secondarily to the frequency with which they change. That is, since the corpus is generally large, it is usually hosted in on a fixed set of machines (usually a single machine with backups), but pieces of it are dispatched to the machines on which the code is hosted on an as-needed basis (that is, via a read from a database and/or some sort of local cacheing); since code is smaller and changes relatively slowly, it is installed onto and tends to stay on a particular box for long periods; since each request is unique (and small), it is routed around farther and to a variety of boxes.
Typically, this means that a box or fleet of boxes are dedicated to a particular purpose. Software is installed onto the server fleet. A load balancer -- which is usually a custom built, dedicated piece of hardware -- has a hand-maintained map indicating how to route requests to the appropriate service fleet. When a request arrives, the service code knows how to fetch (or otherwise have provided) the portion of the corpus on which it needs to operate; makes any necessary updates; and then sends the response.
As Amazon has grown into a global company, we have created datacenters which are geographically close to the customers they serve -- that is, there are datacenters in North America and in Europe. We have discovered that the latency for getting (or updating) the fragments of the corpus we need to the service code can be too long to be useful. As such, the idea of data "locality" -- that is, for making sure that the appropriate pieces of the corpus are available at least within the datacenter, and sometimes down to the rack into which the server is installed, or even on the box itself, becomes an important consideration.
One area of active research and development, however, is into ways to break this traditional structure into more dynamic, adaptive methodologies. Ways to distribute the corpus across multiple machines, ideally with high probability of data locality; ways to unify the software installation and request routing steps; and longer term, ways to adaptively install (or uninstall) the software as request rates demand more local capacity.

## Expected scale points
Scaling is all about bottlenecks. One thing I can guarantee you is that you'll be thinking about the parts of your system that can choke, either before they burst into flame -- or after.
As with all software engineering, service requests fundamentally break down into two basic categories: requests for data (read) and updates to the data (writes). This leads us to five basic dials:

   * total volume of read requests (read rate)
   * total volume of data spewed out as a consequence (read volume)
   * total volume of write requests (write rate)
   * total volume of data spewed at your service as a consequence (write volume)
   * total overall size of the cumulative data set your service will manage (corpus)

So think about scale on a, uh, a scale of one to ten. You're probably used to thinking about it at maybe a five or six. Thinking about scale at Amazon will basically require that you decide which of the dials you'll need to have cranked past 11 and well into the 29s. The scaling techniques you'll apply will depend on which combination of those dials will peg.
The tools at our disposal fall into two basic categories: caching and partitioning. Caching is a good technique to handle very high read rates; some form of partitioning is required to handle scale issues for any of the other knobs.
However, before we dig into these specifics, there's an important concept you need to have in mind.

## CAP Theorem
In short: Consistency, Availability, Partition-tolerance: pick two!
The key observation here is that by not fixating on consistency, particularly in situations that the customer either won't notice or won't care, we can give often simplify our scaling problem a lot.

## Consistency techniques
Odds are that you're already familiar with ACID transactional semantics, and in distributed systems with ideas such as two phase commit. Many scary smart people have thought really long about how to do this; and it turns out it's very, very hard. So at Amazon, we do pretty much whatever we can to avoid systems which require anything even pretending to be two phase commit.
The most common technique is to replace it with some form of optimistic lock.
However, in some of the cutting edge systems, this is replaced with versioning and late reconciliation -- that is, simply apply changes without any locking at all, but instead with enough versioning to be able to figure out the relationship between more or less concurrent updates, and thereby allowing the data to be inconsistent. Then later, when inconsistencies are detected, the data can be reconciled by a variety of techniques -- although reconciliation techniques remains an area of ongoing research.

## Caching techniques
Caching basically means keeping a copy of the data you'll likely need soon somewhere that you can get it fast. Because you'll be operating on a local copy, it is possible (likely, even) that you'll introduce a period immediately after updates during which the data may be inconsistent.
Caching falls into two categories: partial caches, which have enough data available that most requests can be served immediately; and full caches, which have enough data available that all requests can be served immediately.
Partial caches -- of which read through caches are the most common -- work by establishing limits to how much data will be stored, and then throwing away old data when it is determined to be too old, or when space is needed to hold new data. The most common techniques for throwing away data are "time to live" (TTL) and "least recently used" (LRU).
Full caches require that when data is written, the cache be notified in some way so the data can be updated. One technique for doing this is to "write through" the cache, such that the cache actually perform the update. Another way is to have a notification mechanism which triggers a read from the cache; since this two step process ends up moving the same amount of data, generally the notification itself contains a copy of the record to be added to the cache.

## Partitioning techniques
While partitioning is a pretty simple concept, in practice it gets pretty tricky pretty quickly. The concept is just to take the traffic and subdivide it so a given fleet only deals with a subset of the requests.
There are two things to think about:
Partition keyThe data must have some consistent identifier on which to divide the requests and/or corpus.RepartitioningServices typically grow larger over time, and the current partitioning scheme has to be rebalanced.
Typically partitioning is accomplished via a logical/physical partition map. That is:

   * A hash function is applied to the partition key, giving a very large (but not unreasonably large) logical partition space;
   * Each value in the logical partition space is assigned a specific value in the (much smaller) physical space.

For example, the customer ID space might be partitioned into 1024 logical partitions, with only two physical partitions.
Repartitioning then requires calculating a new map, replicating the data into the new physical partitions, applying the new map to the requests, and then eliminating the data from the prior physical partition.
Note that while this discussion focused on the "typical" case, which implies that the data is partitioned by some ubiquitous attribute -- eg, a customer id -- it is also possible and in some applications appropriate to partition the data by time. That is, as each new record appears, put it in the "current" partition; when the current partition fills up, make a new "current" and note the current time range in the partition map. An example of when this might be appropriate would be for a "blog" service; each new posting would go into the current partition, which could be replicated broadly to achieve data locality for the most frequent read requests; because requests for historical posts are much less common, they would likely need much less dedicated hardware (and could suffer higher latency).

## Recovery Oriented Computing
All of these techniques, while interesting and vital, are hard to get right. Three key observations have influenced the direction of most of Amazon's recent infrastructure development.

   * Software bugs contribute significantly to downtime.
   * Hard partitions cause a lot of operational load.
   * Monitoring and intervention cause even more.

The typical first reaction to some of the discussion above of relaxed consistency (eg, for TTL caches introducing a period of inconsistency) is that there are ways to prevent it. "Just notify the cache and take the stuff out when we update it!" And of course, that is viable. However, it is important to consider the complexity it introduces to the system. Over time, more and more exception cases creep into the code, and it quickly becomes delicate. Worse, as the code becomes complex, the failure cases become harder to predict, and the consequences are increasingly dire.
Now let's turn our attention to partitioning. Typically, the logical/physical map must be maintained by hand. That is, the act of re-partitioning consists of deploying a new, uniquely named fleet of servers, and then updating the mapping, copying the data, and so on. Each of these steps is done independently, with a custom tool. Not something you want to do often. And "never" is as not often as it gets.
Finally, consider what it takes to keep a server running. Eventually, every server Amazon has deployed will fail -- repeatedly -- and ultimately fail completely or become obsolete and be replaced. While that may not seem significant at first blush, remember that we are talking about more than 10,000 machines. A mean time between failure of several months seems reasonable -- except that if you are running a fleet of several hundred machines, that means that multiple servers for which you are responsible are likely to be failing every day. Or night. Late at night.
Bringing these three ideas together, we get to a straightforward, powerful, critical guiding principal: Failure is normal. Our software must be designed to be self-healing. This means that simplicity is a core value; you want your system to be as simple as you can make it, even if it means sacrificing some apparent availability -- because the reality is that accepting a predictable 0.1% downtime can often save us 1% in the long haul. Further, our service architecture should allow addition and removal of hosts at will, without any human intervention, and scaling should be a simple matter of adding more hosts.
It's a grand vision, and to be frank we have not yet achieved it. But we're getting closer.
