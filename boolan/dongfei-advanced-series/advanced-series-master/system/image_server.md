ImageServerDesign

This is a good architecture/design question for an SDE II/III candidate. An SDE II candidate should be able to get all the high level features, and have a good start on the low-level implementation details. An SDE III candidate should laugh you out of the building for asking such a question, then launch into a detailed description of various attempts at distributed file systems throughout the ages...
The question is to design an image server that can handle both high throughput and capacity. I usually lay out the specifications for the SITB image server: 300 images/second served at peak, with a total capacity of 60TB. If the candidate is very intimidated by this, I ask them to start with a capacity of 100 GB and a throughput of 10 images/second, and work up from there. The only requirement is that the image server be able to take a request for an image (identified uniquely by an ASIN and a page number) and return the associated JPEG.
Some candidates have trouble figuring out where to start. If that is the case, I write out the general data flow. First a customer requests a page, which is served with an embedded img tag. The img tag has a "url" parameter that contains an ASIN and page number for the requested image. That usually gets them on the right track that the image server has an HTTP front end.
The final design should have the following elements:

   * A set of front-end, lightweight web servers behind a load balancer (that acts as the primary gateway for the service). These web servers don't need significant disk or RAM, but do need fast network interfaces.
   * The front end web servers are responsible for decoding the requests (bonus points if the interviewee adds encryption, and especially if they use expiring URLs).
   * The web servers need to be able to locate the requested image. The right way to do this is generally with a small database, either embedded or RDBMS. If they use an embedded database (CDB, BDB), they need to specify how they are planning to keep them all in sync, and how they can guarantee it. If they use an RDBMS, you need them to tell you how its going to scale (generally using caching of some sort on the client side).
   * Next there is a fleet of back-end file servers with large disk arrays. Bonus points for a discussion of RAID levels here. You should probably look them up on wikipedia before asking this question. We went with RAID 5 for SITB, but opinions differ on whether that was the right solution or not. It's not as important that they choose a particular RAID solution, as that they are able to discuss the merits of using RAID at all.
   * They must specify the communication protocol between the front and back end servers. NFS is a terrible choice, and HTTP is a good choice. They are free to design their own, of course, but I think that is inferior to using a well known protocol (even FTP is fine here, as long as they can adequately describe how they would use it).
   * The back end servers should have VERY lightweight front ends. They shouldn't need to do any processing on the incoming request parameters. Ideally, they will just have Lighttpd set up with a document root serving static files. The less protocol overhead the better.
   * They must include redundancy and load balancing. This is most easily done with load balancers in front of multiple back end mirrors. That way the front end computes the correct partition to which to send the request, then the load balancer takes care of sending it to one of many mirrors in that partition. This is a very important part of the design.
   * Finally, they should include some insight into hot-spots, caching (we didn't use caching, but that is, again, a controversial decision).

A lot of people will attempt to solve this problem using a hash function. They will want to have the front end web servers locate the image by using a fixed hashing function. This is generally a bad solution, because any extra servers added to the fleet cause everything to be re-hashed, and NO ONE wants to move 60TB of images around.
Sometimes they will mention using a SAN, which you should steer them away from (mostly due to the tremendous cost).
Distributed file systems are a growing area of research, and there have been some big ones released and popularized lately (Google's GFS, for instance). So I generally expect candidates for SDE II/III positions to mention the isomorphism between a large scale image server and a generic distributed file system. This can be a fertile (and fun!) vein to explore, if you have a good candidate.
